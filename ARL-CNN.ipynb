{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log file\n",
    "model_id = 8\n",
    "title = 'arl50'\n",
    "log = utils.Logger(verbose=True, title=title)\n",
    "log.logger.info(\"1-1 ARL50-{}\".format(model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of img data\n",
    "pth_train = 'Data/ISIC2017/Aug_Training_Data'\n",
    "pth_valid = 'Data/ISIC2017/ISIC-2017_Validation_Data'\n",
    "pth_test = 'Data/ISIC2017/ISIC-2017_Test_Data'\n",
    "\n",
    "\n",
    "ann_train = utils.Annotation('Data/ISIC2017/ISIC-2017_Training_Aug_Part3_GroundTruth.csv')\n",
    "ann_valid = utils.Annotation('Data/ISIC2017/ISIC-2017_Validation_Part3_GroundTruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "# augmentation transform\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.2), ratio=(3/4, 4/3)),\n",
    "                                      transforms.RandomAffine(10, scale=(0.9, 1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "train_data = utils.RandomPatch(ann_train.df, pth_train, transform=transform_train)\n",
    "train_loader = data.DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "# create validation dataset\n",
    "valid_data = utils.RandomPatch(ann_valid.df, pth_valid, transform=transform)\n",
    "valid_loader = data.DataLoader(valid_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nets\n",
    "\n",
    "model = nets.arlnet50(pretrained=True, num_classes=3)\n",
    "# 1-2\n",
    "# model = nets.resnet50(pretrained=True, num_classes=3)\n",
    "# # 2\n",
    "# model = nets.arlnet18(pretrained=False, num_classes=3)\n",
    "\n",
    "# model = utils.load_model(device, name=\"arl50.pkl\")\n",
    "# model = utils.load_state_dict(model, device, name=\"arl50_dict.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Criterion\n",
    "\n",
    "Focal Loss\n",
    "$$\n",
    "{\\text{FL}(p_{t}) = - \\alpha_t (1 - p_{t})^\\gamma \\log\\left(p_{t}\\right)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting weights\n",
    "# # 2-1/3\n",
    "# class_weight = [1 for _ in range(ann_train.class_num)]\n",
    "# 2-2/4\n",
    "# class_nums = ann_train.count_samples()\n",
    "# class_weight = [len(ann_train.df)/(len(class_nums)*num) for num in class_nums]\n",
    "class_weight = [2.5, 2.5, 0.5]\n",
    "\n",
    "# 2-1/2\n",
    "criterion = utils.FocalLoss(alpha=class_weight, gamma=2, num_classes=ann_train.class_num, reduction='sum')\n",
    "# # 2-3/4\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weight).to(device), reduction='sum')\n",
    "\n",
    "log.logger.info(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test\n",
    "eval_metrics = utils.Evaluation(device, ann_train.categories, 0)\n",
    "eval_metrics.get_probs(model, valid_loader)\n",
    "eval_metrics.complete_scores()\n",
    "log.logger.info(\"Initial Test: valid_acc = {:.4f}, valid_bacc = {:.4f}, f1_score = {}, macro_auc = {}\".format(eval_metrics.acc, eval_metrics.b_acc, eval_metrics.f1_score, eval_metrics.roc_auc['macro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The mini-batch SGD algorithm with a batch size of 32 was adopted as the optimizer. The learning rate was initialized to\n",
    "0.01 for training ARL-CNN14 from scratch and 0.0001 for fine-tuning ARL-CNN50 with pre-trained parameters, and was\n",
    "reduced by half very 30 epochs. The initial weighting factor of the attention feature maps was set to 0.001 in each ARL block\n",
    "when fine-tuning the ARL-CNN50. The maximum epoch number was set to 100.\n",
    "\n",
    "We used the officially provided validation set to monitor the performance of our model and stopped the training process when the network fell into overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, max_epoch=100, test_period=5, early_threshold=5):\n",
    "    \"\"\" train with a scheduler on learning rate\n",
    "\n",
    "    Args:\n",
    "        test_period (int): period of test\n",
    "        early_threshold (int): threshold for early stoppig strategy, which pays attention to acc on test set\n",
    "    \"\"\"\n",
    "    N_train = len(train_loader.dataset)\n",
    "    patience = early_threshold\n",
    "    \n",
    "    # make sure the model is in the training mode\n",
    "    model.train()\n",
    "\n",
    "    global epoch\n",
    "    for epoch in range(epoch, max_epoch):\n",
    "        cost = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            # setting GPU\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cost += loss.item()\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y).sum().item()\n",
    "            \n",
    "        cost = cost / N_train\n",
    "        costs.append(cost)    # average cost\n",
    "        \n",
    "\n",
    "        # ! acc on train in train mode\n",
    "        acc = correct / N_train\n",
    "        train_accs.append(acc)\n",
    "\n",
    "        # adjsut learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        if epoch % test_period == 0:\n",
    "            eval_metrics.get_probs(model, test_loader)\n",
    "            eval_metrics.complete_scores()\n",
    "            test_accs.append(eval_metrics.acc)\n",
    "            b_accs.append(eval_metrics.b_acc)\n",
    "            f1_scores.append(eval_metrics.f1_score)\n",
    "            auc = eval_metrics.roc_auc['macro']\n",
    "            auces.append(auc)\n",
    "            \n",
    "            if auc > eval_metrics.best_score:\n",
    "                eval_metrics.best_score = auc\n",
    "                patience = early_threshold\n",
    "                # 在测试集上准确率上升时，保存模型参数\n",
    "                utils.save_state_dict(model, name=\"{}_dict_{}.pth\".format(title, model_id))\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    break\n",
    "\n",
    "            # (每个epoch）保存checkpoint\n",
    "            utils.check_train(log, model, optimizer, epoch, scheduler, pth_check='ch_train_{}.pth'.format(title))\n",
    "            utils.check_eval(log, costs, train_accs, test_accs, b_accs, f1_scores, auces, pth_check='ch_eval_{}.pth'.format(title), verbose=False)\n",
    "                \n",
    "            log.logger.info(\"{:3d} cost: {:.4f}\\ttrain_acc: {:.4f}\\ttest_acc: {:.4f}\\ttest_bacc: {:.4f}\\tf1_score: {}\\tauc: {:.4f}\".format(\n",
    "                epoch, cost, acc, test_accs[-1], b_accs[-1], f1_scores[-1], auces[-1]))\n",
    "\n",
    "            # change back to training mode    \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "## filename\n",
    "model_file = '{}-{}.pkl'.format(title, model_id)\n",
    "\n",
    "## hyper-params\n",
    "init_lr = 1e-4\n",
    "weight_decay = 1e-2\n",
    "max_epoch = 100\n",
    "test_period = 1\n",
    "early_threshold = 20\n",
    "\n",
    "## optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "## learning rate decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "# 初始化训练状态\n",
    "epoch = utils.load_train(log, model, optimizer, scheduler)\n",
    "costs, train_accs, test_accs, b_accs, f1_scores, auces = utils.load_eval(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load from checkpoint 从checkpoint加载继续训练\n",
    "# epoch = utils.load_train(log, model, optimizer, scheduler, pth_check='ch_train_{}.pth'.format(title))\n",
    "# costs, train_accs, test_accs, b_accs, f1_scores, auces = utils.load_eval(log, pth_check='ch_eval_{}.pth'.format(title))\n",
    "\n",
    "log.logger.info(\"Training...\\n{}\\n{}\".format(optimizer, scheduler))\n",
    "train(model, train_loader, valid_loader, max_epoch, test_period, early_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save checkpoint\n",
    "# utils.check_train(log, model, optimizer, epoch, scheduler)\n",
    "# utils.check_eval(log, costs, train_accs, test_accs, b_accs, f1_scores, auces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "utils.save_model(model, name=model_file)\n",
    "log.logger.info(\"Filename: {}\\ncosts = {}\\ntrain_accs = {}\\ntest_acc = {}\\nb_accs = {}\\nf1_scores = {}\\nauces = {}\".format(\n",
    "    model_file, costs, train_accs, test_accs, b_accs, f1_scores, auces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Implemented in [Evaluation.ipynb](Evaluation.ipynb)\n",
    "\n",
    "In the test stage, we used the same patch extraction method to randomly crop nine patches from each test image, fed them to the trained network, and averaged the obtained scores as the predicted score of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "del train_loader, valid_loader\n",
    "\n",
    "ann_test = utils.Annotation('Data/ISIC2017/ISIC-2017_Test_v2_Part3_GroundTruth.csv')\n",
    "test_data = utils.RandomPatch(ann_test.df, pth_test, transform=transform)\n",
    "test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = utils.Evaluation(device, ann_test.categories)\n",
    "\n",
    "\n",
    "def evaluation_report(model, data_loader):\n",
    "    y, prob = eval_metrics.get_probs(model, data_loader)\n",
    "    probs = [prob]\n",
    "    for _ in range(8):\n",
    "        _, prob = eval_metrics.get_probs(model, data_loader)\n",
    "        probs.append(prob)\n",
    "    prob = np.mean(probs, axis=0)\n",
    "    eval_metrics.prob = prob\n",
    "    \n",
    "    eval_metrics.complete_scores()\n",
    "    \n",
    "    log.logger.info(\"Evaluation on test set\\n{}\".format(eval_metrics.report))\n",
    "    print(\"MEL Acc: {:.4f}\".format(eval_metrics.mel_acc))\n",
    "    print(\"SK Acc: {:.4f}\".format(eval_metrics.sk_acc))\n",
    "    roc_auc = eval_metrics.roc_auc\n",
    "    \n",
    "    for key in roc_auc.keys():\n",
    "        log.logger.info(\"AUC({}): {}\".format(key, roc_auc[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06bb3c3ef4a880548ecfc5dc8501d178eb29b729421bcd01924945e985691c69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
